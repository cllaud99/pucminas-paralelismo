# Trabalho: Fundamentos de Python â€” Paralelismo  
**PUC Minas - Engenharia de Dados**  
**Disciplina:** Fundamentos de Python  
**Aluno:** ClÃ¡udio Pontes  

---

### ğŸ“Œ DescriÃ§Ã£o  
Este projeto foi desenvolvido como parte da disciplina de Fundamentos de Python com foco em tÃ©cnicas de paralelismo e concorrÃªncia. O objetivo Ã© demonstrar o uso de threads e processos para acelerar tarefas tÃ­picas de Engenharia de Dados, como ingestÃ£o de arquivos, chamadas a APIs, transformaÃ§Ã£o de dados, simulaÃ§Ãµes de ETL e benchmarking de performance.

O projeto estÃ¡ dividido em trÃªs nÃ­veis de complexidade:

- **NÃ­vel 1 â€” Fundamentos com foco em I/O**  
- **NÃ­vel 2 â€” Processamento CPU-bound com dados**  
- **NÃ­vel 3 â€” CenÃ¡rios avanÃ§ados e integraÃ§Ãµes**

---

### ğŸ§  ExercÃ­cios implementados  
Cada nÃ­vel contÃ©m 5 exercÃ­cios numerados conforme a proposta da disciplina:

**level_01/**  
1. Crawler concorrente de APIs usando threading.Thread para coletar dados de mÃºltiplos endpoints.

2. IngestÃ£o paralela de vÃ¡rios arquivos CSV com ThreadPoolExecutor e Pandas.

3. Monitoramento simultÃ¢neo de tempo de resposta de diversas URLs, com gravaÃ§Ã£o dos resultados.

4. SimulaÃ§Ã£o de downloads concorrentes usando ThreadPoolExecutor e delays artificiais.

5. Consulta paralela simulada a mÃºltiplas bases de dados com threads, gerando resumos independentes.

**level_02/**

6. TransformaÃ§Ã£o pesada de dados usando ProcessPoolExecutor para operaÃ§Ãµes CPU-bound.
7. AplicaÃ§Ã£o paralela de funÃ§Ãµes complexas em partes de DataFrames com processos.
8. ConversÃ£o paralela de arquivos Parquet para CSV, processando cada arquivo individualmente.
9. CÃ¡lculo paralelo de agregaÃ§Ãµes estatÃ­sticas (soma, mÃ©dia, desvio) em grandes DataFrames.
10. Pipeline multiprocessamento: transformaÃ§Ã£o em um processo e persistÃªncia em outro, usando pipes ou queues.

**level_03/**

11. ETL paralelo por partiÃ§Ã£o de dados, simulando processamento distribuÃ­do em Data Lake.
12. ExtraÃ§Ã£o concorrente de dados paginados via API, unindo resultados em paralelo com threads.
13. SimulaÃ§Ã£o de mÃºltiplos produtores e consumidores usando queue.Queue para leitura e escrita concorrentes.
14. Benchmark comparativo entre pipeline serial e paralela em trÃªs etapas (ingestÃ£o, transformaÃ§Ã£o, escrita).
15. Sistema paralelizado de alertas baseado em monitoramento de logs e detecÃ§Ã£o de padrÃµes crÃ­ticos.

---

### ğŸ“ Estrutura de pastas  
```markdown
.
â”œâ”€â”€ README.md
â”œâ”€â”€ data/ # Dados de entrada e saÃ­da
â”œâ”€â”€ logs/ # Arquivos de log gerados pelos exercÃ­cios
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ pyproject.toml
â””â”€â”€ src/
    â”œâ”€â”€ apis/
    â”‚   â””â”€â”€ weather_api.py
    â”œâ”€â”€ level_01/
    â”‚   â””â”€â”€ exercice_01.py ... exercice_05.py
    â”œâ”€â”€ level_02/
    â”‚   â””â”€â”€ exercice_06.py ... exercice_10.py
    â”œâ”€â”€ level_03/
    â”‚   â””â”€â”€ exercice_11.py ... exercice_15.py
    â”œâ”€â”€ utils/
    â”‚   â”œâ”€â”€ faker_create_datasets.py
    â”‚   â”œâ”€â”€ simulated_api.py
    â”‚   â”œâ”€â”€ compare_times.py
    â”‚   â””â”€â”€ log_decorator.py
    â””â”€â”€ main.py # Script principal para orquestrar todos os nÃ­veis
```

---

### â–¶ï¸ Como executar  

Gere uma chave de api em: weatherapi.com (importante para o exercicio 01 somente)

Instale o poetry:   
https://python-poetry.org/docs/#installation


Clone o repositÃ³rio:

```bash
git clone https://github.com/seu-usuario/pucminas-paralelismo.git
cd pucminas-paralelismo
```

Instale as dependÃªncias (via Poetry):

```bash
poetry install
```

Execute o projeto:

```bash
poetry run python src/main.py
```

Este comando:

- Gera datasets simulados com faker_create_datasets.py  
- Inicia uma API local simulada (FastAPI)  
- Executa todos os exercÃ­cios dos trÃªs nÃ­veis em sequÃªncia  

---

### ğŸ›  Principais bibliotecas utilizadas

- **Python 3.12**  
  VersÃ£o moderna e atual da linguagem, garantindo suporte a recursos recentes para melhor desempenho e produtividade.

- **requests**  
  Biblioteca para realizar chamadas HTTP, essencial para a comunicaÃ§Ã£o com APIs externas e internas.

- **python-dotenv**  
  Para gerenciamento seguro e prÃ¡tico de variÃ¡veis de ambiente, facilitando a configuraÃ§Ã£o do projeto.

- **taskipy**  
  Ferramenta para orquestraÃ§Ã£o e execuÃ§Ã£o de tarefas, ajudando na automaÃ§Ã£o dos comandos do projeto.

- **loguru**  
  Biblioteca poderosa e simples para logging, utilizada junto com decoradores para monitoramento e debug eficientes.

- **pandas**  
  Biblioteca fundamental para manipulaÃ§Ã£o e anÃ¡lise de dados, usada nas transformaÃ§Ãµes e processamento dos datasets.

- **faker**  
  GeraÃ§Ã£o de dados sintÃ©ticos realistas para testes e simulaÃ§Ãµes, garantindo flexibilidade nos exercÃ­cios.

- **fastapi**  
  Framework moderno e rÃ¡pido para criaÃ§Ã£o da API simulada, suportando chamadas concorrentes.

- **uvicorn**  
  Servidor ASGI leve para rodar a API FastAPI localmente durante a execuÃ§Ã£o do projeto.

- **aiomultiprocess**  
  Biblioteca para facilitar o uso de multiprocessamento assÃ­ncrono, aumentando a eficiÃªncia no processamento paralelo.

---

### âœ… Task de formataÃ§Ã£o  
```bash
poetry run task format
```

Executa o isort e black para manter o cÃ³digo limpo e padronizado.

---

### ğŸŒ API Simulada  
Durante a execuÃ§Ã£o, uma API FastAPI Ã© iniciada localmente na porta 8574, simulando endpoints para exercÃ­cios com chamadas HTTP concorrentes.

---

### ğŸ“Œ ObservaÃ§Ãµes  
- Todos os exercÃ­cios foram projetados com foco em boas prÃ¡ticas de Engenharia de Dados.  
- O projeto utiliza logs para acompanhamento e debug, os logs foram versionados para demostrar a funcionalidade do exercicio 15
- SimulaÃ§Ãµes foram aplicadas para representar cenÃ¡rios reais de ETL e monitoramento.
